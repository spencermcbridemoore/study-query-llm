{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Study Query LLM - PCA KLLMeans Analysis & Visualization\n",
    "\n",
    "This notebook loads previously saved sweep results from pickle files and provides:\n",
    "1. Visualization of stability metrics across K values and summarizers\n",
    "2. Data comparison and validation\n",
    "3. Representative text comparison across summarizers\n",
    "4. Custom analysis of results\n",
    "\n",
    "**Prerequisites:** Run `pca_kllmeans_sweep.ipynb` first to generate pickle files."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install dependencies (REQUIRED - Run this first!)\n",
    "\n",
    "**IMPORTANT:** You must install the package dependencies before running the notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Install the package in editable mode\n",
    "%pip install -e ..\n",
    "\n",
    "# After installation, restart the kernel (Kernel -> Restart Kernel) and run cells from the top"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Results from Pickle File"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import numpy as np\n",
    "from pathlib import Path\n",
    "\n",
    "# Option 1: Specify the exact filename\n",
    "# saved_file = \"pca_kllmeans_sweep_results_20260204_231913.pkl\"\n",
    "\n",
    "# Option 2: Load the most recent pickle file (default)\n",
    "pickle_files = sorted(Path(\".\").glob(\"pca_kllmeans_sweep_results_*.pkl\"), reverse=True)\n",
    "if pickle_files:\n",
    "    saved_file = pickle_files[0]\n",
    "    print(f\"[INFO] Loading results from: {saved_file}\")\n",
    "    \n",
    "    with open(saved_file, \"rb\") as f:\n",
    "        loaded_data = pickle.load(f)\n",
    "    \n",
    "    # Handle both old format (dict of summarizers) and new format (with metadata)\n",
    "    if \"summarizers\" in loaded_data:\n",
    "        # New format with metadata\n",
    "        results = loaded_data[\"summarizers\"]\n",
    "        ground_truth_labels = loaded_data.get(\"ground_truth_labels\")\n",
    "        dataset_name = loaded_data.get(\"dataset_name\", \"unknown\")\n",
    "        print(f\"[OK] Loaded new format: {len(results)} summarizer(s), dataset: {dataset_name}\")\n",
    "        if ground_truth_labels is not None:\n",
    "            ground_truth_labels = np.array(ground_truth_labels)  # Convert back to numpy array\n",
    "            print(f\"   Ground truth: {len(ground_truth_labels)} samples, {len(set(ground_truth_labels))} clusters\")\n",
    "        else:\n",
    "            print(f\"   Ground truth: None\")\n",
    "    else:\n",
    "        # Old format (backward compatible)\n",
    "        results = loaded_data\n",
    "        ground_truth_labels = None\n",
    "        dataset_name = \"unknown\"\n",
    "        print(f\"[OK] Loaded old format: {len(results)} summarizer(s) (backward compatible)\")\n",
    "    \n",
    "    # Show structure\n",
    "    if results:\n",
    "        first_key = list(results.keys())[0]\n",
    "        first_data = results[first_key]\n",
    "        if 'by_k' in first_data:\n",
    "            k_values = sorted([int(k) for k in first_data['by_k'].keys()])\n",
    "            print(f\"   K values: {k_values}\")\n",
    "            print(f\"   Example access: results['{first_key}']['by_k']['{k_values[0]}']['stability']\")\n",
    "else:\n",
    "    print(\"[ERROR] No pickle files found. Run pca_kllmeans_sweep.ipynb first to generate results.\")\n",
    "    results = {}\n",
    "    ground_truth_labels = None\n",
    "    dataset_name = \"unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from typing import Any, Dict, Set, Tuple\n",
    "\n",
    "\n",
    "def analyze_dict_structure(\n",
    "    data: Any,\n",
    "    indent: int = 0,\n",
    "    seen_structures: Dict[Tuple[Tuple[type, ...], Tuple[type, ...]], int] = None,\n",
    "    path: str = \"root\"\n",
    ") -> None:\n",
    "    \"\"\"\n",
    "    Analyze and print the structure of a nested dictionary.\n",
    "    \n",
    "    For each dictionary level, shows:\n",
    "    - Number of dictionaries with that structure\n",
    "    - Key types\n",
    "    - Value types (recursively if values are dictionaries)\n",
    "    \n",
    "    Args:\n",
    "        data: The dictionary (or any data) to analyze\n",
    "        indent: Current indentation level (for recursive calls)\n",
    "        seen_structures: Dictionary tracking structure counts (internal use)\n",
    "        path: Current path in the structure (for debugging)\n",
    "    \"\"\"\n",
    "    if seen_structures is None:\n",
    "        seen_structures = defaultdict(int)\n",
    "    \n",
    "    if not isinstance(data, dict):\n",
    "        return\n",
    "    \n",
    "    # Collect key and value types for this dictionary\n",
    "    key_types: Set[type] = set()\n",
    "    value_types: Set[type] = set()\n",
    "    nested_dicts = []\n",
    "    \n",
    "    for key, value in data.items():\n",
    "        key_types.add(type(key))\n",
    "        \n",
    "        value_type = type(value)\n",
    "        value_types.add(value_type)\n",
    "        \n",
    "        if isinstance(value, dict):\n",
    "            nested_dicts.append((key, value))\n",
    "    \n",
    "    # Create a signature for this structure\n",
    "    key_types_tuple = tuple(sorted(key_types, key=lambda t: t.__name__))\n",
    "    value_types_tuple = tuple(sorted(value_types, key=lambda t: t.__name__))\n",
    "    structure_sig = (key_types_tuple, value_types_tuple)\n",
    "    \n",
    "    # Count this structure\n",
    "    seen_structures[structure_sig] += 1\n",
    "    count = seen_structures[structure_sig]\n",
    "    \n",
    "    # Print this level's structure\n",
    "    indent_str = \"  \" * indent\n",
    "    print(f\"{indent_str}Level {indent}: {count} dict(s) with this structure\")\n",
    "    print(f\"{indent_str}  Keys: {' | '.join(t.__name__ for t in sorted(key_types, key=lambda t: t.__name__))}\")\n",
    "    \n",
    "    # Format value types, marking dicts specially\n",
    "    value_type_names = []\n",
    "    for vt in sorted(value_types, key=lambda t: t.__name__):\n",
    "        if vt == dict:\n",
    "            value_type_names.append(\"dict (nested)\")\n",
    "        else:\n",
    "            value_type_names.append(vt.__name__)\n",
    "    print(f\"{indent_str}  Values: {' | '.join(value_type_names)}\")\n",
    "    \n",
    "    # Recursively process nested dictionaries\n",
    "    if nested_dicts:\n",
    "        print(f\"{indent_str}  Nested dictionaries:\")\n",
    "        for key, nested_dict in nested_dicts:\n",
    "            print(f\"{indent_str}    -> '{key}' (dict)\")\n",
    "            analyze_dict_structure(\n",
    "                nested_dict,\n",
    "                indent=indent + 2,\n",
    "                seen_structures=seen_structures,\n",
    "                path=f\"{path}.{key}\"\n",
    "            )\n",
    "\n",
    "\n",
    "def print_dict_structure(data: Dict) -> None:\n",
    "    \"\"\"\n",
    "    Convenience wrapper to print dictionary structure starting from root.\n",
    "    \n",
    "    Args:\n",
    "        data: The dictionary to analyze\n",
    "    \"\"\"\n",
    "    print(\"Dictionary Structure Analysis:\")\n",
    "    print(\"=\" * 50)\n",
    "    analyze_dict_structure(data)\n",
    "    print(\"=\" * 50)\n",
    "\n",
    "\n",
    "example = {\n",
    "    \"a\": 1,\n",
    "    \"b\": \"string\",\n",
    "    \"c\": {\n",
    "        \"d\": 2,\n",
    "        \"e\": {\n",
    "            \"f\": 3,\n",
    "            \"g\": \"nested_string\"\n",
    "        }\n",
    "    },\n",
    "    \"h\": {\n",
    "        \"i\": 4\n",
    "    }\n",
    "}\n",
    "\n",
    "print_dict_structure(example)\n",
    "print_dict_structure(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Visualize Stability Metrics\n",
    "\n",
    "Create visualizations of stability metrics across K values and summarizers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "if results:\n",
    "    metrics_to_plot = [\n",
    "        (\"silhouette\", \"Cosine Silhouette (mean ± std)\"),\n",
    "        (\"stability_ari\", \"Stability ARI (mean ± std)\"),\n",
    "        (\"dispersion\", \"Reconstruction Error / point (mean ± std)\"),\n",
    "        (\"coverage\", \"Coverage Fraction (mean ± std)\"),\n",
    "    ]\n",
    "    \n",
    "    # Get coverage threshold from saved data if available\n",
    "    coverage_threshold = 0.2  # Default, update if stored in results\n",
    "    \n",
    "    fig, axes = plt.subplots(2, 2, figsize=(14, 10), sharex=True)\n",
    "    axes = axes.flatten()\n",
    "    \n",
    "    # Use different line styles and markers to distinguish overlapping lines\n",
    "    line_styles = ['-', '--', '-.', ':']\n",
    "    markers = ['o', 's', '^', 'D', 'v', 'p', '*', 'h']\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(results)))\n",
    "    \n",
    "    for ax, (metric_key, title) in zip(axes, metrics_to_plot):\n",
    "        plotted_count = 0\n",
    "        for idx, summarizer_name in enumerate(sorted(results.keys())):\n",
    "            try:\n",
    "                k_data = results[summarizer_name][\"by_k\"]\n",
    "                ks = sorted([int(k) for k in k_data.keys() if k_data[k].get(\"stability\")])\n",
    "                \n",
    "                if not ks:\n",
    "                    continue\n",
    "                    \n",
    "                means = []\n",
    "                stds = []\n",
    "                valid_ks = []\n",
    "                \n",
    "                for k in ks:\n",
    "                    stability = k_data[str(k)].get(\"stability\")\n",
    "                    if stability and metric_key in stability:\n",
    "                        means.append(stability[metric_key][\"mean\"])\n",
    "                        stds.append(stability[metric_key][\"std\"])\n",
    "                        valid_ks.append(k)\n",
    "                \n",
    "                if valid_ks:\n",
    "                    # Use different styles for each summarizer\n",
    "                    style_idx = idx % len(line_styles)\n",
    "                    marker_idx = idx % len(markers)\n",
    "                    color = colors[idx]\n",
    "                    \n",
    "                    ax.plot(\n",
    "                        valid_ks, means, \n",
    "                        marker=markers[marker_idx],\n",
    "                        linestyle=line_styles[style_idx],\n",
    "                        label=summarizer_name, \n",
    "                        linewidth=2.5,\n",
    "                        color=color,\n",
    "                        markersize=6,\n",
    "                        markeredgewidth=1.5\n",
    "                    )\n",
    "                    ax.fill_between(\n",
    "                        valid_ks,\n",
    "                        np.array(means) - np.array(stds),\n",
    "                        np.array(means) + np.array(stds),\n",
    "                        alpha=0.15,\n",
    "                        color=color\n",
    "                    )\n",
    "                    plotted_count += 1\n",
    "            except Exception as e:\n",
    "                print(f\"[WARN]  Error plotting {summarizer_name} for {metric_key}: {e}\")\n",
    "                continue\n",
    "        \n",
    "        if plotted_count == 0:\n",
    "            print(f\"[WARN]  No data to plot for {metric_key}\")\n",
    "        \n",
    "        ax.set_title(title, fontsize=11, fontweight='bold')\n",
    "        ax.set_xlabel(\"K (number of clusters)\", fontsize=10)\n",
    "        ax.set_ylabel(metric_key.replace('_', ' ').title(), fontsize=10)\n",
    "        ax.grid(True, alpha=0.3)\n",
    "        ax.legend(fontsize=8)\n",
    "    \n",
    "    # Add overall title\n",
    "    title_parts = [f\"Stability Metrics vs K for Different Summarizers\"]\n",
    "    if 'dataset_name' in globals() and dataset_name != \"unknown\":\n",
    "        title_parts.append(f\"Dataset: {dataset_name}\")\n",
    "    title_parts.append(f\"Coverage threshold: {coverage_threshold}\")\n",
    "    \n",
    "    fig.suptitle(\n",
    "        \" | \".join(title_parts),\n",
    "        fontsize=12,\n",
    "        fontweight='bold',\n",
    "        y=0.995\n",
    "    )\n",
    "    \n",
    "    plt.tight_layout(rect=[0, 0, 1, 0.97])\n",
    "    plt.show()\n",
    "    \n",
    "    print(f\"\\n[OK] Visualized {len(results)} summarizer(s) across K values\")\n",
    "else:\n",
    "    print(\"[WARN]  No results to visualize\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ground Truth Comparison (if available)\n",
    "\n",
    "Compare predicted cluster labels against ground truth labels using Adjusted Rand Index (ARI)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ground truth comparison visualization\n",
    "if 'ground_truth_labels' in globals() and ground_truth_labels is not None:\n",
    "    from study_query_llm.algorithms.clustering import adjusted_rand_index\n",
    "    \n",
    "    print(f\"[INFO] Computing ground truth ARI for dataset: {dataset_name}\")\n",
    "    print(f\"   Ground truth clusters: {len(set(ground_truth_labels))}\")\n",
    "    \n",
    "    # Compute ARI for each K and summarizer\n",
    "    gt_ari_data = {}\n",
    "    for summarizer_name in sorted(results.keys()):\n",
    "        gt_ari_data[summarizer_name] = {}\n",
    "        k_data = results[summarizer_name][\"by_k\"]\n",
    "        ks = sorted([int(k) for k in k_data.keys() if k_data[k].get(\"labels\") is not None])\n",
    "        \n",
    "        for k in ks:\n",
    "            labels = np.array(k_data[str(k)][\"labels\"])\n",
    "            if len(labels) == len(ground_truth_labels):\n",
    "                ari = adjusted_rand_index(labels, ground_truth_labels)\n",
    "                gt_ari_data[summarizer_name][k] = ari\n",
    "    \n",
    "    # Create visualization\n",
    "    fig, ax = plt.subplots(1, 1, figsize=(10, 6))\n",
    "    \n",
    "    # Plot ARI vs K for each summarizer\n",
    "    line_styles = ['-', '--', '-.', ':']\n",
    "    markers = ['o', 's', '^', 'D', 'v', 'p', '*', 'h']\n",
    "    colors = plt.cm.tab10(np.linspace(0, 1, len(gt_ari_data)))\n",
    "    \n",
    "    for idx, summarizer_name in enumerate(sorted(gt_ari_data.keys())):\n",
    "        ks = sorted(gt_ari_data[summarizer_name].keys())\n",
    "        aris = [gt_ari_data[summarizer_name][k] for k in ks]\n",
    "        \n",
    "        style_idx = idx % len(line_styles)\n",
    "        marker_idx = idx % len(markers)\n",
    "        color = colors[idx]\n",
    "        \n",
    "        ax.plot(\n",
    "            ks, aris,\n",
    "            marker=markers[marker_idx],\n",
    "            linestyle=line_styles[style_idx],\n",
    "            label=summarizer_name,\n",
    "            linewidth=2.5,\n",
    "            color=color,\n",
    "            markersize=8,\n",
    "            markeredgewidth=1.5\n",
    "        )\n",
    "    \n",
    "    # Add vertical line at true number of clusters\n",
    "    true_k = len(set(ground_truth_labels))\n",
    "    ax.axvline(x=true_k, color='red', linestyle='--', linewidth=2, alpha=0.7, label=f'True K={true_k}')\n",
    "    \n",
    "    ax.set_xlabel(\"K (number of clusters)\", fontsize=11)\n",
    "    ax.set_ylabel(\"Adjusted Rand Index vs Ground Truth\", fontsize=11)\n",
    "    ax.set_title(f\"Ground Truth Comparison: {dataset_name}\", fontsize=12, fontweight='bold')\n",
    "    ax.grid(True, alpha=0.3)\n",
    "    ax.legend(fontsize=9)\n",
    "    ax.set_ylim([-0.1, 1.1])\n",
    "    \n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "    \n",
    "    # Print best K for each summarizer\n",
    "    print(f\"\\n[INFO] Best K by ground truth ARI:\")\n",
    "    for summarizer_name in sorted(gt_ari_data.keys()):\n",
    "        best_k = max(gt_ari_data[summarizer_name].items(), key=lambda x: x[1])\n",
    "        print(f\"   {summarizer_name}: K={best_k[0]} (ARI={best_k[1]:.3f})\")\n",
    "    \n",
    "else:\n",
    "    print(\"[INFO] No ground truth labels available for this dataset.\")\n",
    "    print(\"   Ground truth comparison requires benchmark datasets with known cluster labels.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare representatives for a specific K value\n",
    "K_TO_COMPARE = 5\n",
    "\n",
    "print(f\"\\n{'='*60}\")\n",
    "print(f\"Comparing representatives for K={K_TO_COMPARE}\")\n",
    "print(f\"{'='*60}\")\n",
    "\n",
    "for summarizer_name in sorted(results.keys()):\n",
    "    if str(K_TO_COMPARE) in results[summarizer_name][\"by_k\"]:\n",
    "        reps = results[summarizer_name][\"by_k\"][str(K_TO_COMPARE)].get(\"representatives\", [])\n",
    "        print(f\"\\n{summarizer_name}:\")\n",
    "        for i, rep in enumerate(reps, 1):\n",
    "            print(f\"  {i}. {rep[:120]}{'...' if len(rep) > 120 else ''}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Test: Check if Data is Identical or Just Very Close\n",
    "\n",
    "Test whether summarizers have truly identical data (bit-for-bit) or if values are just very close (within floating-point precision)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "if results:\n",
    "    print(f\"\\n{'='*60}\")\n",
    "    print(\"PRECISION TEST: Identical vs Very Close\")\n",
    "    print(f\"{'='*60}\\n\")\n",
    "    \n",
    "    # Test different tolerance levels\n",
    "    tolerances = [\n",
    "        (1e-15, \"Machine epsilon (1e-15)\"),\n",
    "        (1e-12, \"Very tight (1e-12)\"),\n",
    "        (1e-10, \"Tight (1e-10)\"),\n",
    "        (1e-8, \"Moderate (1e-8)\"),\n",
    "        (1e-6, \"Loose (1e-6)\"),\n",
    "    ]\n",
    "    \n",
    "    # Get all summarizers\n",
    "    summarizers = sorted(results.keys())\n",
    "    \n",
    "    if len(summarizers) < 2:\n",
    "        print(\"[WARN]  Need at least 2 summarizers to compare\")\n",
    "    else:\n",
    "        # Test for a specific K value across all metrics\n",
    "        test_k = \"5\"\n",
    "        metrics_to_test = [\"silhouette\", \"stability_ari\", \"dispersion\", \"coverage\"]\n",
    "        \n",
    "        print(f\"Testing K={test_k} across {len(summarizers)} summarizers\\n\")\n",
    "        \n",
    "        # Collect all metric values\n",
    "        metric_data = {}\n",
    "        for metric_key in metrics_to_test:\n",
    "            metric_data[metric_key] = {}\n",
    "            for name in summarizers:\n",
    "                k_data = results[name][\"by_k\"]\n",
    "                if test_k in k_data and k_data[test_k].get(\"stability\"):\n",
    "                    stability = k_data[test_k][\"stability\"]\n",
    "                    if metric_key in stability:\n",
    "                        metric_data[metric_key][name] = stability[metric_key][\"mean\"]\n",
    "        \n",
    "        # Test each metric\n",
    "        for metric_key in metrics_to_test:\n",
    "            if not metric_data[metric_key]:\n",
    "                continue\n",
    "                \n",
    "            print(f\"\\n{metric_key.upper().replace('_', ' ')}:\")\n",
    "            print(\"-\" * 60)\n",
    "            \n",
    "            values = list(metric_data[metric_key].values())\n",
    "            if len(values) < 2:\n",
    "                print(\"  [WARN]  Not enough data to compare\")\n",
    "                continue\n",
    "            \n",
    "            # Check if all values are exactly equal (bit-for-bit identical)\n",
    "            all_identical = all(v == values[0] for v in values)\n",
    "            \n",
    "            if all_identical:\n",
    "                print(f\"  [OK] BIT-FOR-BIT IDENTICAL: All values = {values[0]}\")\n",
    "                print(f\"     This means the data is truly identical, not just close\")\n",
    "            else:\n",
    "                # Calculate differences\n",
    "                min_val = min(values)\n",
    "                max_val = max(values)\n",
    "                diff = max_val - min_val\n",
    "                rel_diff = diff / abs(min_val) if min_val != 0 else diff\n",
    "                \n",
    "                print(f\"  Values: {[f'{v:.12f}' for v in values]}\")\n",
    "                print(f\"  Range: [{min_val:.12f}, {max_val:.12f}]\")\n",
    "                print(f\"  Absolute difference: {diff:.2e}\")\n",
    "                print(f\"  Relative difference: {rel_diff:.2e}\")\n",
    "                \n",
    "                # Test against each tolerance\n",
    "                print(f\"\\n  Tolerance tests:\")\n",
    "                for tol, desc in tolerances:\n",
    "                    within_tol = diff < tol\n",
    "                    status = \"[OK]\" if within_tol else \"[ERROR]\"\n",
    "                    print(f\"    {status} {desc}: {'WITHIN' if within_tol else 'EXCEEDS'} tolerance\")\n",
    "                \n",
    "                # Determine if \"very close\" or \"different\"\n",
    "                if diff < 1e-12:\n",
    "                    print(f\"\\n  [INFO] CONCLUSION: Values are VERY CLOSE (likely same computation, minor FP differences)\")\n",
    "                elif diff < 1e-8:\n",
    "                    print(f\"\\n  [INFO] CONCLUSION: Values are CLOSE (may be same computation with some variation)\")\n",
    "                else:\n",
    "                    print(f\"\\n  [INFO] CONCLUSION: Values are DIFFERENT (likely different computations)\")\n",
    "        \n",
    "        # Additional test: Check if all metrics are identical across all K values\n",
    "        print(f\"\\n{'='*60}\")\n",
    "        print(\"ACROSS-ALL-K TEST\")\n",
    "        print(f\"{'='*60}\\n\")\n",
    "        \n",
    "        all_k_identical = True\n",
    "        all_k_very_close = True\n",
    "        \n",
    "        for k in sorted([int(k) for k in results[summarizers[0]][\"by_k\"].keys()]):\n",
    "            k_str = str(k)\n",
    "            for metric_key in metrics_to_test:\n",
    "                values = []\n",
    "                for name in summarizers:\n",
    "                    k_data = results[name][\"by_k\"]\n",
    "                    if k_str in k_data and k_data[k_str].get(\"stability\"):\n",
    "                        stability = k_data[k_str][\"stability\"]\n",
    "                        if metric_key in stability:\n",
    "                            values.append(stability[metric_key][\"mean\"])\n",
    "                \n",
    "                if len(values) > 1:\n",
    "                    # Check if identical\n",
    "                    if not all(v == values[0] for v in values):\n",
    "                        all_k_identical = False\n",
    "                    # Check if very close\n",
    "                    if max(values) - min(values) >= 1e-10:\n",
    "                        all_k_very_close = False\n",
    "        \n",
    "        if all_k_identical:\n",
    "            print(\"[OK] ALL metrics are BIT-FOR-BIT IDENTICAL across all K values and summarizers\")\n",
    "            print(\"   This confirms the data is truly identical (same computation)\")\n",
    "        elif all_k_very_close:\n",
    "            print(\"[OK] ALL metrics are VERY CLOSE (< 1e-10) across all K values and summarizers\")\n",
    "            print(\"   Values are essentially identical (likely same computation with minor FP differences)\")\n",
    "        else:\n",
    "            print(\"[WARN]  Some metrics differ significantly across K values or summarizers\")\n",
    "            print(\"   This suggests there may be actual differences in the computations\")\n",
    "else:\n",
    "    print(\"[WARN]  No results to test\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python (vastai-ssh-jupyter-pytorch-env)",
   "language": "python",
   "name": "vastai-ssh-jupyter-pytorch-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
