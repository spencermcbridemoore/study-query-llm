{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# Study Query LLM - PCA KLLMeans Sweep (Jupyter)\n",
        "\n",
        "This notebook:\n",
        "1) Loads prompt texts from the dictionary\n",
        "2) Fetches/stores embeddings using `EmbeddingService` (v2 schema)\n",
        "3) Runs the PCA KLLMeans sweep using the `algorithms` library\n",
        "4) Tests multiple LLM summarizers (3 LLMs + None) for stability analysis\n",
        "5) Tracks provenance using `ProvenanceService`\n",
        "\n",
        "**Note:** This is the Jupyter version (not Colab-specific). For Colab, use `colab_pca_kllmeans_sweep.ipynb`."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Install dependencies (if needed)\n",
        "\n",
        "Uncomment if running in a fresh environment:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# %pip install openai python-dotenv sqlalchemy psycopg2-binary nest_asyncio tqdm numpy\n",
        "# Note: python-dotenv is optional - the notebook will work without it"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Configure environment\n",
        "\n",
        "Set your environment variables or load from `.env` file:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "ename": "ModuleNotFoundError",
          "evalue": "No module named 'dotenv'",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mModuleNotFoundError\u001b[39m                       Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[2]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mos\u001b[39;00m\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34;01mdotenv\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m load_dotenv\n\u001b[32m      4\u001b[39m \u001b[38;5;66;03m# Load from .env file if it exists\u001b[39;00m\n\u001b[32m      5\u001b[39m load_dotenv()\n",
            "\u001b[31mModuleNotFoundError\u001b[39m: No module named 'dotenv'"
          ]
        }
      ],
      "source": [
        "import os\n",
        "\n",
        "# Try to load from .env file if python-dotenv is available\n",
        "try:\n",
        "    from dotenv import load_dotenv\n",
        "    load_dotenv()\n",
        "    print(\"✅ Loaded environment variables from .env file (if present)\")\n",
        "except ImportError:\n",
        "    print(\"ℹ️  python-dotenv not installed. Skipping .env file loading.\")\n",
        "    print(\"   Install with: pip install python-dotenv\")\n",
        "    print(\"   Or set environment variables directly below.\")\n",
        "\n",
        "# Set environment variables (or use .env file)\n",
        "os.environ.setdefault(\"AZURE_OPENAI_API_KEY\", \"your-azure-api-key\")\n",
        "os.environ.setdefault(\"AZURE_OPENAI_ENDPOINT\", \"https://your-resource.openai.azure.com/\")\n",
        "os.environ.setdefault(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\", \"text-embedding-3-small\")\n",
        "os.environ.setdefault(\"AZURE_OPENAI_API_VERSION\", \"2024-02-15-preview\")\n",
        "os.environ.setdefault(\"AZURE_OPENAI_DEPLOYMENT\", \"gpt-4o-mini\")\n",
        "os.environ.setdefault(\n",
        "    \"DATABASE_URL\",\n",
        "    \"postgresql://username:password@host:port/database?sslmode=require\"\n",
        ")\n",
        "\n",
        "print(\"\\nEnvironment variables:\")\n",
        "print(\"AZURE_OPENAI_ENDPOINT:\", os.environ.get(\"AZURE_OPENAI_ENDPOINT\"))\n",
        "print(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT:\", os.environ.get(\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"))\n",
        "print(\"AZURE_OPENAI_DEPLOYMENT:\", os.environ.get(\"AZURE_OPENAI_DEPLOYMENT\"))\n",
        "print(\"DATABASE_URL set:\", bool(os.environ.get(\"DATABASE_URL\")))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Initialize database and services"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "import time\n",
        "import asyncio\n",
        "import nest_asyncio\n",
        "import numpy as np\n",
        "from tqdm import tqdm\n",
        "from study_query_llm.db.connection_v2 import DatabaseConnectionV2\n",
        "from study_query_llm.db.raw_call_repository import RawCallRepository\n",
        "from study_query_llm.services.embedding_service import EmbeddingService, EmbeddingRequest\n",
        "from study_query_llm.services.summarization_service import SummarizationService, SummarizationRequest\n",
        "from study_query_llm.services.provenance_service import ProvenanceService\n",
        "from study_query_llm.algorithms import SweepConfig, run_sweep\n",
        "\n",
        "nest_asyncio.apply()\n",
        "\n",
        "# Initialize DB connection (creates tables if needed)\n",
        "db = DatabaseConnectionV2(os.environ[\"DATABASE_URL\"], enable_pgvector=True)\n",
        "db.init_db()\n",
        "\n",
        "# Get embedding deployment\n",
        "embedding_deployment = os.environ[\"AZURE_OPENAI_EMBEDDING_DEPLOYMENT\"]\n",
        "print(f\"Using embedding deployment: {embedding_deployment}\")\n",
        "print(\"✅ Database and services ready (will be created per-operation)\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Prompt dictionary\n",
        "\n",
        "Load your prompt dictionary here. You can:\n",
        "1. Define it directly in the cell below\n",
        "2. Load from a pickle file: `database_estela_dict = pickle.load(open('file.pkl', 'rb'))`\n",
        "3. Load from a JSON file: `import json; database_estela_dict = json.load(open('file.json', 'r'))`"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Option 1: Define directly\n",
        "# database_estela_dict = {\n",
        "#     'path/to/file.yaml': {\n",
        "#         'generation prompts': [\n",
        "#             {'prompt 1': 'Your prompt text here...'},\n",
        "#         ],\n",
        "#     },\n",
        "# }\n",
        "\n",
        "# Option 2: Load from pickle\n",
        "# import pickle\n",
        "# database_estela_dict = pickle.load(open('your_dict.pkl', 'rb'))\n",
        "\n",
        "# Option 3: Load from JSON\n",
        "# import json\n",
        "# database_estela_dict = json.load(open('your_dict.json', 'r', encoding='utf-8'))\n",
        "\n",
        "# For now, create empty dict - you must define it above\n",
        "database_estela_dict = {}"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Flatten prompts and extract texts"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Flatten nested prompt dictionary into a flat map of key tuples -> prompt strings\n",
        "\n",
        "def _is_prompt_key(key: str) -> bool:\n",
        "    key_lower = key.lower()\n",
        "    return \"prompt\" in key_lower  # catches \"prompt\", \"prompt id\", \"prompt #\", etc\n",
        "\n",
        "\n",
        "def flatten_prompt_dict(data, path=()):\n",
        "    flat = {}\n",
        "\n",
        "    if isinstance(data, dict):\n",
        "        for key, value in data.items():\n",
        "            new_path = path + (key,)\n",
        "            if isinstance(key, str) and _is_prompt_key(key) and isinstance(value, str):\n",
        "                flat[new_path] = value\n",
        "            else:\n",
        "                flat.update(flatten_prompt_dict(value, new_path))\n",
        "    elif isinstance(data, list):\n",
        "        for i, value in enumerate(data):\n",
        "            new_path = path + (f\"[{i}]\",)\n",
        "            flat.update(flatten_prompt_dict(value, new_path))\n",
        "\n",
        "    return flat\n",
        "\n",
        "\n",
        "# Flatten prompts and extract texts\n",
        "flat_prompts = flatten_prompt_dict(database_estela_dict)\n",
        "texts = list(flat_prompts.values())\n",
        "\n",
        "# Filter out empty/invalid strings (EmbeddingService will reject them anyway)\n",
        "def _clean_texts(texts_list: list[str]) -> list[str]:\n",
        "    \"\"\"Clean and filter texts.\"\"\"\n",
        "    cleaned = []\n",
        "    for text in texts_list:\n",
        "        if text is None:\n",
        "            continue\n",
        "        if not isinstance(text, str):\n",
        "            text = str(text)\n",
        "        text = text.replace(\"\\x00\", \"\").strip()\n",
        "        if text:  # Only keep non-empty strings\n",
        "            cleaned.append(text)\n",
        "    return cleaned\n",
        "\n",
        "\n",
        "texts = _clean_texts(texts)\n",
        "print(f\"✅ Flattened {len(texts)} valid prompts\")\n",
        "\n",
        "# Show a few samples\n",
        "for i, (k, v) in enumerate(list(flat_prompts.items())[:3]):\n",
        "    print(f\"\\nSample {i+1} - Key: {k}\")\n",
        "    print(f\"  Text: {v[:150]}{'...' if len(v) > 150 else ''}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Fetch embeddings using EmbeddingService"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "async def fetch_embeddings_async(texts_list: list[str], deployment: str) -> np.ndarray:\n",
        "    \"\"\"Fetch or create embeddings using EmbeddingService.\"\"\"\n",
        "    with db.session_scope() as session:\n",
        "        repo = RawCallRepository(session)\n",
        "        service = EmbeddingService(repository=repo)\n",
        "\n",
        "        # Create embedding requests\n",
        "        requests = [\n",
        "            EmbeddingRequest(text=text, deployment=deployment)\n",
        "            for text in texts_list\n",
        "        ]\n",
        "\n",
        "        # Get embeddings (will use cache if available)\n",
        "        responses = await service.get_embeddings_batch(requests)\n",
        "\n",
        "        # Extract vectors\n",
        "        embeddings = [resp.vector for resp in responses]\n",
        "\n",
        "        return np.asarray(embeddings, dtype=np.float64)\n",
        "\n",
        "\n",
        "# Fetch embeddings\n",
        "print(f\"Fetching embeddings for {len(texts)} texts using {embedding_deployment}...\")\n",
        "embeddings = await fetch_embeddings_async(texts, embedding_deployment)\n",
        "print(f\"✅ Got embeddings: shape {embeddings.shape}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Run PCA KLLMeans Sweep with Multiple LLM Summarizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Define LLM deployments for summarization (3 LLMs + None = 4 runs)\n",
        "llm_summarizers = [\n",
        "    None,  # Non-LLM summaries (just use original representatives)\n",
        "    \"gpt-4o-mini\",\n",
        "    \"gpt-4o\",\n",
        "    \"gpt-5-chat-2025-08-07\",  # Add your preferred third LLM here\n",
        "]\n",
        "\n",
        "# Configure sweep: k=2 to k=10\n",
        "# Enable stability metrics with multiple restarts\n",
        "cfg = SweepConfig(\n",
        "    pca_dim=64,\n",
        "    rank_r=2,\n",
        "    k_min=2,\n",
        "    k_max=10,\n",
        "    max_iter=200,\n",
        "    base_seed=0,\n",
        "    n_restarts=20,  # Multiple restarts for stability analysis\n",
        "    compute_stability=True,  # Enable stability metrics (silhouette, ARI, dispersion, coverage)\n",
        "    coverage_threshold=0.2,  # Cosine distance threshold for coverage metric\n",
        ")\n",
        "\n",
        "# Helper to create paraphraser using SummarizationService\n",
        "def create_paraphraser_for_llm(llm_deployment: str):\n",
        "    \"\"\"Create a synchronous paraphraser function for a specific LLM deployment.\"\"\"\n",
        "    if llm_deployment is None:\n",
        "        return None\n",
        "\n",
        "    async def _paraphrase_batch_async(texts: list[str]) -> list[str]:\n",
        "        \"\"\"Async wrapper for summarization.\"\"\"\n",
        "        with db.session_scope() as session:\n",
        "            repo = RawCallRepository(session)\n",
        "            service = SummarizationService(repository=repo)\n",
        "\n",
        "            request = SummarizationRequest(\n",
        "                texts=texts,\n",
        "                llm_deployment=llm_deployment,\n",
        "                temperature=0.2,\n",
        "                max_tokens=128,\n",
        "            )\n",
        "\n",
        "            result = await service.summarize_batch(request)\n",
        "            return result.summaries\n",
        "\n",
        "    def paraphrase_batch_sync(texts: list[str]) -> list[str]:\n",
        "        \"\"\"Synchronous wrapper for run_sweep.\"\"\"\n",
        "        try:\n",
        "            loop = asyncio.get_event_loop()\n",
        "            if loop.is_running():\n",
        "                # If loop is running, we need to use nest_asyncio\n",
        "                return loop.run_until_complete(_paraphrase_batch_async(texts))\n",
        "        except RuntimeError:\n",
        "            pass\n",
        "        return asyncio.run(_paraphrase_batch_async(texts))\n",
        "\n",
        "    return paraphrase_batch_sync\n",
        "\n",
        "\n",
        "# Store all results\n",
        "all_results = {}\n",
        "\n",
        "# Create run group for provenance tracking\n",
        "with db.session_scope() as session:\n",
        "    repo = RawCallRepository(session)\n",
        "    provenance = ProvenanceService(repository=repo)\n",
        "\n",
        "    run_group_id = provenance.create_run_group(\n",
        "        name=f\"pca_kllmeans_sweep_{embedding_deployment}\",\n",
        "        metadata={\n",
        "            \"embedding_deployment\": embedding_deployment,\n",
        "            \"n_texts\": len(texts),\n",
        "            \"k_range\": f\"{cfg.k_min}-{cfg.k_max}\",\n",
        "            \"llm_summarizers\": [s if s else \"None\" for s in llm_summarizers],\n",
        "        },\n",
        "    )\n",
        "    print(f\"✅ Created run group: id={run_group_id}\")\n",
        "\n",
        "# Run sweep for each LLM summarizer\n",
        "for llm_deployment in tqdm(llm_summarizers, desc=\"LLM Summarizers\"):\n",
        "    summarizer_name = \"None\" if llm_deployment is None else llm_deployment\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Running sweep with summarizer: {summarizer_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    # Create paraphraser\n",
        "    paraphraser = create_paraphraser_for_llm(llm_deployment)\n",
        "\n",
        "    # Run sweep\n",
        "    result = run_sweep(texts, embeddings, cfg, paraphraser=paraphraser)\n",
        "    all_results[summarizer_name] = result\n",
        "\n",
        "    print(f\"✅ Completed. Ks: {sorted([int(k) for k in result.by_k.keys()])}\")\n",
        "\n",
        "# Summary\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(\"SUMMARY\")\n",
        "print(f\"{'='*60}\")\n",
        "print(f\"Embedding deployment: {embedding_deployment}\")\n",
        "print(f\"Number of texts: {len(texts)}\")\n",
        "print(f\"K range: {cfg.k_min} to {cfg.k_max}\")\n",
        "print(\n",
        "    f\"Summarizers tested: {len(llm_summarizers)} ({', '.join([s if s else 'None' for s in llm_summarizers])})\"\n",
        ")\n",
        "print(f\"\\nResults structure: all_results[summarizer_name]['by_k'][k_value]\")\n",
        "print(f\"\\nExample access:\")\n",
        "print(f\"  all_results['None']['by_k']['5']['representatives']\")\n",
        "if any(s for s in llm_summarizers if s):\n",
        "    first_llm = next(s for s in llm_summarizers if s)\n",
        "    print(f\"  all_results['{first_llm}']['by_k']['5']['representatives']\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## View Results"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Display results for each summarizer\n",
        "for summarizer_name, result in all_results.items():\n",
        "    print(f\"\\n{'='*60}\")\n",
        "    print(f\"Results for summarizer: {summarizer_name}\")\n",
        "    print(f\"{'='*60}\")\n",
        "\n",
        "    for k in sorted([int(k) for k in result.by_k.keys()]):\n",
        "        k_data = result.by_k[str(k)]\n",
        "        reps = k_data.get(\"representatives\", [])\n",
        "        print(f\"\\nK={k}: {len(reps)} representatives\")\n",
        "        for i, rep in enumerate(reps[:3], 1):  # Show first 3\n",
        "            print(f\"  {i}. {rep[:100]}{'...' if len(rep) > 100 else ''}\")\n",
        "        if len(reps) > 3:\n",
        "            print(f\"  ... and {len(reps) - 3} more\")\n",
        "        \n",
        "        # Show stability metrics if available\n",
        "        if k_data.get(\"stability\"):\n",
        "            stab = k_data[\"stability\"]\n",
        "            print(f\"    Silhouette: {stab['silhouette']['mean']:.3f} ± {stab['silhouette']['std']:.3f}\")\n",
        "            print(f\"    Stability ARI: {stab['stability_ari']['mean']:.3f} ± {stab['stability_ari']['std']:.3f}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Save Results (Complete - All Metrics + Matrices)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Save results to a pickle file for later analysis\n",
        "# Includes all metrics, labels, objectives, and distance matrices for recomputation\n",
        "import pickle\n",
        "from datetime import datetime\n",
        "\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "output_file = f\"pca_kllmeans_sweep_results_{timestamp}.pkl\"\n",
        "\n",
        "# Prepare results for saving (convert numpy arrays to lists for pickle compatibility)\n",
        "save_results = {}\n",
        "for summarizer_name, result in all_results.items():\n",
        "    save_results[summarizer_name] = {\n",
        "        \"pca\": result.pca,\n",
        "        \"by_k\": {},\n",
        "    }\n",
        "\n",
        "    # Save distance matrices for later metric computation\n",
        "    if result.Z is not None:\n",
        "        save_results[summarizer_name][\"Z\"] = result.Z.tolist()\n",
        "    if result.Z_norm is not None:\n",
        "        save_results[summarizer_name][\"Z_norm\"] = result.Z_norm.tolist()\n",
        "    if result.dist is not None:\n",
        "        save_results[summarizer_name][\"dist\"] = result.dist.tolist()\n",
        "\n",
        "    # Save all data for each K value\n",
        "    for k, k_data in result.by_k.items():\n",
        "        save_results[summarizer_name][\"by_k\"][k] = {\n",
        "            \"representatives\": k_data.get(\"representatives\", []),\n",
        "            \"labels\": (\n",
        "                k_data.get(\"labels\", []).tolist()\n",
        "                if hasattr(k_data.get(\"labels\"), \"tolist\")\n",
        "                else k_data.get(\"labels\", [])\n",
        "            ),\n",
        "            \"labels_all\": (\n",
        "                [\n",
        "                    l.tolist() if hasattr(l, \"tolist\") else l\n",
        "                    for l in k_data.get(\"labels_all\", [])\n",
        "                ]\n",
        "                if k_data.get(\"labels_all\") is not None\n",
        "                else None\n",
        "            ),\n",
        "            \"objective\": k_data.get(\"objective\", {}),\n",
        "            \"objectives\": k_data.get(\"objectives\", []),  # All restart objectives\n",
        "            \"stability\": k_data.get(\"stability\"),  # Stability metrics (silhouette, ARI, dispersion, coverage)\n",
        "        }\n",
        "\n",
        "with open(output_file, \"wb\") as f:\n",
        "    pickle.dump(save_results, f)\n",
        "\n",
        "print(f\"✅ Results saved to: {output_file}\")\n",
        "print(\n",
        "    f\"   Includes: representatives, labels, objectives, stability metrics, and distance matrices\"\n",
        ")\n",
        "print(f\"   To load: results = pickle.load(open('{output_file}', 'rb'))\")\n",
        "print(f\"\\n   Example access:\")\n",
        "print(f\"     results['None']['by_k']['5']['stability']['silhouette']['mean']\")\n",
        "print(f\"     results['None']['by_k']['5']['representatives']\")\n",
        "print(f\"     results['None']['dist']  # Distance matrix for recomputing metrics\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Compare Representatives Across Summarizers"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Compare representatives for a specific K value\n",
        "K_TO_COMPARE = 5\n",
        "\n",
        "print(f\"\\n{'='*60}\")\n",
        "print(f\"Comparing representatives for K={K_TO_COMPARE}\")\n",
        "print(f\"{'='*60}\")\n",
        "\n",
        "for summarizer_name in sorted(all_results.keys()):\n",
        "    if str(K_TO_COMPARE) in all_results[summarizer_name].by_k:\n",
        "        reps = all_results[summarizer_name].by_k[str(K_TO_COMPARE)].get(\n",
        "            \"representatives\", []\n",
        "        )\n",
        "        print(f\"\\n{summarizer_name}:\")\n",
        "        for i, rep in enumerate(reps, 1):\n",
        "            print(f\"  {i}. {rep[:120]}{'...' if len(rep) > 120 else ''}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Notes\n",
        "\n",
        "- All embeddings are cached in the database (v2 schema: `RawCall` + `EmbeddingVector`)\n",
        "- All LLM summarization calls are logged to `RawCall` with full provenance\n",
        "- Results are stored in `all_results[summarizer_name]['by_k'][k_value]`\n",
        "- **Stability metrics enabled**: 20 restarts per K with full metrics (silhouette, ARI, dispersion, coverage)\n",
        "- **Complete data saved**: All metrics, labels, objectives, and distance matrices saved to pickle for later analysis\n",
        "- You can load the pickle file later and recompute or analyze metrics without re-running the sweep"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Load and Analyze Saved Results (Later)\n",
        "\n",
        "Use this cell to load previously saved results and analyze metrics without re-running the sweep."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Load saved results\n",
        "# import pickle\n",
        "# import numpy as np\n",
        "\n",
        "# Replace with your actual pickle filename\n",
        "# saved_file = \"pca_kllmeans_sweep_results_20250203_123456.pkl\"\n",
        "# results = pickle.load(open(saved_file, \"rb\"))\n",
        "\n",
        "# Example: Access stability metrics\n",
        "# for summarizer_name in results.keys():\n",
        "#     print(f\"\\n{summarizer_name}:\")\n",
        "#     for k in sorted([int(k) for k in results[summarizer_name][\"by_k\"].keys()]):\n",
        "#         k_data = results[summarizer_name][\"by_k\"][k]\n",
        "#         if k_data.get(\"stability\"):\n",
        "#             stab = k_data[\"stability\"]\n",
        "#             print(f\"  K={k}:\")\n",
        "#             print(f\"    Silhouette: {stab['silhouette']['mean']:.3f} ± {stab['silhouette']['std']:.3f}\")\n",
        "#             print(f\"    Stability ARI: {stab['stability_ari']['mean']:.3f} ± {stab['stability_ari']['std']:.3f}\")\n",
        "#             print(f\"    Dispersion: {stab['dispersion']['mean']:.3f} ± {stab['dispersion']['std']:.3f}\")\n",
        "#             print(f\"    Coverage: {stab['coverage']['mean']:.3f} ± {stab['coverage']['std']:.3f}\")\n",
        "\n",
        "# Example: Recompute metrics using saved distance matrices\n",
        "# if \"dist\" in results[\"None\"]:\n",
        "#     dist_matrix = np.array(results[\"None\"][\"dist\"])\n",
        "#     labels = np.array(results[\"None\"][\"by_k\"][\"5\"][\"labels\"])\n",
        "#     # Now you can recompute metrics using the algorithms library functions"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python (vastai-ssh-jupyter-pytorch-env)",
      "language": "python",
      "name": "vastai-ssh-jupyter-pytorch-env"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.12.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2
}
