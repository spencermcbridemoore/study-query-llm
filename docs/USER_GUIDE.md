# User Guide - Study Query LLM

## Overview

Study Query LLM is a web application for running LLM inference experiments across multiple providers (Azure OpenAI, OpenAI, Hyperbolic) and analyzing the results. All inference runs are automatically logged to a database for later analysis.

## Getting Started

### Prerequisites

- Python 3.10 or higher
- API keys for at least one LLM provider (Azure OpenAI, OpenAI, or Hyperbolic)
- SQLite (included with Python) or PostgreSQL database

### Installation

1. **Clone the repository**
   ```bash
   git clone <repository-url>
   cd study-query-llm
   ```

2. **Create a virtual environment**
   ```bash
   python -m venv venv
   source venv/bin/activate  # On Windows: venv\Scripts\activate
   ```

3. **Install dependencies**
   ```bash
   pip install -e ".[dev]"
   ```

4. **Configure environment variables**
   
   Create a `.env` file in the project root:
   ```bash
   # Database (SQLite for development, PostgreSQL for production)
   # V1 (Legacy) - SQLite database
   DATABASE_URL=sqlite:///study_query_llm.db
   
   # V2 (PostgreSQL) - New immutable capture schema with grouping support
   # Uncomment to use v2 schema:
   # DATABASE_URL=postgresql://user:password@localhost:5432/study_query_llm_v2
   # To enable pgvector for embedding vectors: CREATE EXTENSION vector;
   
   # Azure OpenAI
   AZURE_OPENAI_API_KEY=your-azure-api-key
   AZURE_OPENAI_ENDPOINT=https://your-resource.openai.azure.com/
   AZURE_OPENAI_DEPLOYMENT=gpt-4o
   AZURE_OPENAI_API_VERSION=2024-02-15-preview
   
   # Azure OpenAI Embedding Deployments (optional)
   # AZURE_OPENAI_EMBEDDING_DEPLOYMENT=text-embedding-3-small
   # AZURE_OPENAI_EMBEDDING_DEPLOYMENTS=text-embedding-3-small,text-embedding-3-large
   
   # OpenAI (optional)
   OPENAI_API_KEY=your-openai-api-key
   OPENAI_MODEL=gpt-4
   
   # Hyperbolic (optional)
   HYPERBOLIC_API_KEY=your-hyperbolic-api-key
   HYPERBOLIC_ENDPOINT=https://api.hyperbolic.xyz
   ```

5. **Initialize the database**
   
   **For V1 (SQLite) schema:**
   ```bash
   python -c "from study_query_llm.db.connection import DatabaseConnection; from study_query_llm.config import config; db = DatabaseConnection(config.database.connection_string); db.init_db()"
   ```
   
   **For V2 (PostgreSQL) schema:**
   ```bash
   python -c "from study_query_llm.db.connection_v2 import DatabaseConnectionV2; import os; db = DatabaseConnectionV2(os.getenv('DATABASE_URL')); db.init_db()"
   ```
   
   **Migrating from V1 to V2:**
   ```bash
   LEGACY_DATABASE_URL=sqlite:///study_query_llm.db \
   DATABASE_URL=postgresql://user:password@localhost:5432/study_query_llm_v2 \
   python scripts/migrate_v1_to_v2.py
   ```

### Running the Application

**Start the Panel web interface:**
```bash
panel serve panel_app/app.py --show
```

The application will open in your default web browser at `http://localhost:5006`.

## Using the Application

### Inference Tab

The Inference tab allows you to run LLM inference experiments.

#### Running an Inference

1. **Select a Provider**
   - Choose from the available providers (Azure, OpenAI, Hyperbolic)
   - Providers with configured API keys will appear in the dropdown

2. **Load Deployments (Azure only)**
   - If using Azure, click "Load Deployments" to see available deployments
   - Select a deployment from the dropdown
   - The deployment name must match what's configured in Azure Portal

3. **Enter Your Prompt**
   - Type your prompt in the text area
   - Prompts can be any length (will be truncated if too long)

4. **Adjust Parameters (Optional)**
   - **Temperature**: Controls randomness (0.0 = deterministic, 2.0 = very random)
   - **Max Tokens**: Maximum number of tokens in the response

5. **Run Inference**
   - Click "Run Inference" button
   - Wait for the response (status will show progress)
   - Results appear below:
     - **Response**: The LLM's generated text
     - **Metadata**: Provider, tokens used, latency, temperature, inference ID

#### Understanding Results

- **Response**: The actual text generated by the LLM
- **Provider**: Which provider was used (e.g., `azure_openai_gpt-4o`)
- **Tokens**: Total tokens used (prompt + response)
- **Latency**: Time taken for the API call in milliseconds
- **Inference ID**: Unique identifier saved in the database

### Analytics Tab

The Analytics tab provides insights into your inference history.

#### Summary Statistics

- **Total Inferences**: Number of inference runs stored
- **Total Tokens**: Cumulative tokens used across all inferences
- **Unique Providers**: Number of different providers used

#### Provider Comparison

A table comparing performance across providers:
- **Provider**: Provider name
- **Count**: Number of inferences
- **Avg Tokens**: Average tokens per inference
- **Avg Latency**: Average response time
- **Total Tokens**: Total tokens used
- **Avg Cost Estimate**: Estimated cost (if available)

#### Recent Inferences

A table showing the most recent inference runs:
- **ID**: Inference ID
- **Prompt**: Truncated prompt text
- **Provider**: Provider used
- **Tokens**: Tokens consumed
- **Latency**: Response time
- **Created At**: Timestamp

#### Refreshing Data

Click the "Refresh" button to update all analytics with the latest data from the database.

## Features

### Automatic Logging

All inference runs are automatically saved to the database with:
- Original prompt
- Generated response
- Provider information
- Token usage
- Latency metrics
- Timestamp
- Optional batch ID for grouping related runs

### Batch Tracking

Inferences can be grouped using batch IDs:
- **Batch Inference**: Run multiple different prompts together
- **Sampling Inference**: Run the same prompt multiple times to get varied responses

Both automatically generate batch IDs for grouping related runs.

### Error Handling

The application includes robust error handling:
- Automatic retry on transient errors (rate limits, timeouts)
- Clear error messages in the UI
- Comprehensive logging for debugging
- Graceful degradation (inference continues even if database save fails)

### Preprocessing Options

The service layer supports optional prompt preprocessing:
- Whitespace normalization
- Truncation of long prompts
- PII removal (emails, phone numbers)
- Control character stripping
- Template application

## Troubleshooting

### "No providers configured" Error

**Problem**: Provider dropdown shows "No providers configured"

**Solution**: 
- Check your `.env` file has the correct API keys
- Verify environment variable names match exactly
- Restart the application after updating `.env`

### "Deployment not found" Error (Azure)

**Problem**: Azure inference fails with "DeploymentNotFound"

**Solution**:
- Click "Load Deployments" to see available deployments
- Select a deployment from the dropdown
- Verify the deployment name matches Azure Portal exactly
- Check your `.env` has the correct `AZURE_OPENAI_DEPLOYMENT` value

### Database Connection Issues

**Problem**: Database errors or connection failures

**Solution**:
- For SQLite: Check file permissions in the project directory
- For PostgreSQL: Verify connection string format: `postgresql://user:password@host:port/dbname`
- Ensure database is initialized: Run the initialization command above

### Slow Performance

**Problem**: Application is slow or unresponsive

**Solution**:
- Check network connection (API calls require internet)
- Review provider API rate limits
- Consider using SQLite for development (faster than PostgreSQL for small datasets)
- Check logs for errors or warnings

## Advanced Usage

### Programmatic Access

You can use the core library programmatically:

```python
from study_query_llm.providers.factory import ProviderFactory
from study_query_llm.services.inference_service import InferenceService
from study_query_llm.config import config

# Create provider
factory = ProviderFactory()
provider = factory.create_from_config("azure")

# Create service
service = InferenceService(provider)

# Run inference
result = await service.run_inference("What is the capital of France?")
print(result['response'])
```

### Batch Operations

Run multiple inferences programmatically:

```python
# Batch inference (different prompts)
prompts = ["What is Python?", "What is JavaScript?", "What is Rust?"]
results = await service.run_batch_inference(prompts)

# Sampling inference (same prompt, multiple times)
results = await service.run_sampling_inference("Say hello", n=5)
```

### Analytics Queries

Query the database directly:

```python
from study_query_llm.db.connection import DatabaseConnection
from study_query_llm.db.inference_repository import InferenceRepository
from study_query_llm.services.study_service import StudyService

db = DatabaseConnection(config.database.connection_string)
with db.session_scope() as session:
    repo = InferenceRepository(session)
    study = StudyService(repo)
    
    # Get provider comparison
    comparison = study.get_provider_comparison()
    print(comparison)
    
    # Get recent inferences
    recent = study.get_recent_inferences(limit=10)
    print(recent)
```

## Logging

The application uses structured logging. Logs are written to:
- **Console**: Standard output (INFO level and above)
- **File**: Optional log file (if configured)

Log levels:
- **DEBUG**: Detailed diagnostic information
- **INFO**: General informational messages
- **WARNING**: Warning messages
- **ERROR**: Error messages with full traceback

To enable debug logging, set the log level in your code:
```python
from study_query_llm.utils.logging_config import setup_logging
import logging

setup_logging(level=logging.DEBUG)
```

## Support

For issues, questions, or contributions:
- Check the [Architecture Documentation](ARCHITECTURE.md)
- Review the [Implementation Plan](IMPLEMENTATION_PLAN.md)
- Open an issue on the repository

